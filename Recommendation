import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from nltk.tokenize import word_tokenize
from datetime import datetime
from transformers import BertTokenizer, BertModel
import torch
from sklearn.preprocessing import MinMaxScaler

# Sample News Data with Additional Fields
news_data = {
    'article_id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'title': [
        "Soccer World Cup Highlights", "AI in Gaming Trends",
        "Space Exploration Breakthroughs", "Soccer Training Tips",
        "Quantum Computing Advances", "Soccer League Updates",
        "AI Innovations in Tech", "Mars Rover Discovery",
        "Soccer Skills Guide", "Quantum Physics in AI"
    ],
    'content': [
        "Exciting soccer matches in the World Cup.", "AI trends shaping gaming industry.",
        "New discoveries in space exploration.", "Improve soccer skills with these expert tips.",
        "Latest research in quantum computing.", "Highlights from recent soccer leagues.",
        "AI innovations transforming technology.", "Discoveries from Mars rover findings.",
        "In-depth soccer training guide.", "Applications of quantum physics in AI."
    ],
    'category': [
        "Sports", "Technology", "Science", "Sports",
        "Technology", "Sports", "Technology", "Science",
        "Sports", "Science"
    ],
    'author': ["Author A", "Author B", "Author C", "Author A", "Author D", "Author A", "Author B", "Author C", "Author A", "Author D"],
    'views': [1500, 2300, 1800, 1200, 2100, 1700, 2500, 2200, 1400, 1600],
    'shares': [120, 200, 180, 90, 210, 160, 240, 230, 130, 150],
    'sentiment': [0.8, 0.7, 0.85, 0.9, 0.75, 0.6, 0.65, 0.8, 0.7, 0.72],
    'publish_date': ["2024-10-01", "2024-09-20", "2024-10-10", "2024-09-25", "2024-08-15", "2024-10-05", "2024-09-30", "2024-09-18", "2024-10-02", "2024-08-20"]
}

news_df = pd.DataFrame(news_data)
news_df['publish_date'] = pd.to_datetime(news_df['publish_date'])

# User Profile Data with interests and watch history
user_data = {
    'user_id': [1, 2, 3],
    'interests': [
        "Soccer, Gaming, Technology", "Space, Quantum Computing, AI",
        "Gaming, Soccer, Science"
    ],
    'watch_history': [
        [1, 4, 6], [3, 5, 8], [2, 4, 7]
    ]
}
user_df = pd.DataFrame(user_data)

# Helper: Text Processing and Embeddings
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertModel.from_pretrained('bert-base-uncased')

def get_bert_embeddings(text):
    inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True)
    outputs = bert_model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).detach().numpy()[0]

news_df['bert_embeddings'] = news_df['title'] + ' ' + news_df['content']
news_df['bert_embeddings'] = news_df['bert_embeddings'].apply(get_bert_embeddings)

# Similarity-based Content Recommendation
def get_content_recommendations(user_interests, top_n=5):
    user_embedding = get_bert_embeddings(user_interests)
    news_embeddings = np.vstack(news_df['bert_embeddings'])
    cosine_sim = cosine_similarity([user_embedding], news_embeddings).flatten()
    news_df['content_score'] = cosine_sim
    return news_df.sort_values(by='content_score', ascending=False)[['article_id', 'title', 'content_score']].head(top_n)

# Collaborative Filtering with Neural Collaborative Filtering (NCF)
def ncf_collaborative_filtering(user_id, top_n=5):
    user_history = user_df.loc[user_df['user_id'] == user_id, 'watch_history'].values[0]
    similar_users = user_df[user_df['watch_history'].apply(lambda x: len(set(x) & set(user_history)) > 0)]
    recommendations = set()
    for history in similar_users['watch_history']:
        recommendations.update(history)
    recommendations -= set(user_history)
    recommended_articles = news_df[news_df['article_id'].isin(recommendations)]
    recommended_articles['collab_score'] = 1
    return recommended_articles[['article_id', 'title', 'collab_score']].head(top_n)

# Freshness, Sentiment, Popularity Weighting
def compute_freshness(publish_date, current_date=datetime.now()):
    days_old = (current_date - publish_date).days
    return max(0, 1 - days_old / 365)

news_df['freshness_score'] = news_df['publish_date'].apply(compute_freshness)
news_df['popularity_score'] = MinMaxScaler().fit_transform(news_df[['views', 'shares']].mean(axis=1).values.reshape(-1, 1))
news_df['sentiment_score'] = news_df['sentiment']

# Hybrid Recommendation System
def recommend_articles(user_id, content_weight=0.4, collab_weight=0.25, freshness_weight=0.15, popularity_weight=0.1, sentiment_weight=0.1, top_n=5):
    user_interests = user_df.loc[user_df['user_id'] == user_id, 'interests'].values[0]
    content_recs = get_content_recommendations(user_interests, top_n=top_n * 2)
    collab_recs = ncf_collaborative_filtering(user_id, top_n=top_n * 2)

    # Merge recommendations
    all_recs = pd.merge(content_recs, collab_recs, on='article_id', how='outer').fillna(0)
    all_recs = pd.merge(all_recs, news_df[['article_id', 'freshness_score', 'popularity_score', 'sentiment_score']], on='article_id', how='left')
    
    all_recs['final_score'] = (
        all_recs['content_score'] * content_weight +
        all_recs['collab_score'] * collab_weight +
        all_recs['freshness_score'] * freshness_weight +
        all_recs['popularity_score'] * popularity_weight +
        all_recs['sentiment_score'] * sentiment_weight
    )
    final_recs = all_recs.sort_values(by='final_score', ascending=False).head(top_n)
    
    print(f"Recommended Articles for User {user_id}:")
    print(final_recs[['article_id', 'title', 'final_score']])
    return final_recs  # Return the recommendations for evaluation

# Evaluation
def evaluate_recommendations(user_id, recommended_articles, relevancy_scores):
    relevancy = relevancy_scores.get(user_id, [])
    true_positive = len(set(recommended_articles['article_id']).intersection(relevancy))
    precision = true_positive / len(recommended_articles)
    recall = true_positive / len(relevancy) if relevancy else 0
    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) else 0
    return {"precision": precision, "recall": recall, "f1_score": f1_score}

# Generate and Evaluate Recommendations
recommended_articles = recommend_articles(user_id=1, content_weight=0.4, collab_weight=0.25, freshness_weight=0.15, popularity_weight=0.1, sentiment_weight=0.1, top_n=5)

# Evaluation Example
relevancy_scores = {
    1: [1, 6, 4]  # Relevant articles for user 1
}

# Evaluate the recommendations based on the relevancy scores
evaluation = evaluate_recommendations(user_id=1, recommended_articles=recommended_articles, relevancy_scores=relevancy_scores)
print(f"Evaluation Metrics for User 1: Precision: {evaluation['precision']:.2f}, Recall: {evaluation['recall']:.2f}, F1 Score: {evaluation['f1_score']:.2f}")
